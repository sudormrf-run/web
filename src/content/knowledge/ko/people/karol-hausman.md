---
title: Karol Hausman
description: Google DeepMind → Physical Intelligence 공동창업자
tags: [karol-hausman, google, deepmind, physical-intelligence, rt, pi0]
category: people
---

# Karol Hausman

> Home > People > Karol Hausman

---

## Profile

| 항목 | 내용 |
|------|------|
| 현직 | Physical Intelligence 공동창업자 |
| 이전 | Google DeepMind Staff Research Scientist |
| PhD | USC (University of Southern California) |
| 국적 | 폴란드 |

---

## 핵심 기여

- **RT 시리즈 핵심 리더**: RT-1, RT-2, RT-X 개발 주도
- **SayCan**: LLM + 로봇 제어 연결의 초기 연구
- **Physical Intelligence 창업**: π0 개발
- **Google Robotics 핵심 인물**: VLA 연구의 산업화 주도

---

## Research Timeline

### PhD & Early Career (2012-2017)

**USC - Stefan Schaal 지도**

| Year | Work | Impact |
|------|------|--------|
| 2015 | Skill Learning | 로봇 스킬 학습 |
| 2017 | Multi-Task Learning | 다중 태스크 로봇 학습 |

### Google Brain / DeepMind (2017-2024)

**Google Robotics 핵심 연구**

| Year | Work | Impact |
|------|------|--------|
| 2018 | 입사 | Google Brain Robotics |
| 2020 | Multi-Task RL | 다중 태스크 학습 |
| 2022 | **SayCan** | LLM + 로봇 grounding |
| 2022 | **RT-1** | Robotics Transformer |
| 2023 | **RT-2** | 첫 VLA 모델 |
| 2023 | **RT-X** | Open X-Embodiment |
| 2023 | **PaLM-E** | Embodied Language Model |

### Physical Intelligence (2024-present)

**공동창업 & π0 개발**

| Year | Work | Impact |
|------|------|--------|
| 2024 | Physical Intelligence 창업 | 범용 로봇 AI |
| 2024 | **π0** | Flow matching VLA |
| 2025 | **π0.5** | Open-world 일반화 |

---

## Major Publications

### VLA & Foundation Models
- **RT-1** (2022) - Robotics Transformer
- **RT-2** (2023) - Vision-Language-Action
- **RT-X** (2023) - Open X-Embodiment
- **PaLM-E** (2023) - Embodied multimodal model
- **π0** (2024) - Flow matching VLA

### LLM + Robotics
- **SayCan** (2022) - LLM grounding to robotics
- Inner Monologue (2022) - LLM feedback for robots

### Multi-Task Learning
- Multi-Task RL (2018)
- Skill Composition (2019)

---

## Key Ideas

### SayCan (2022)
```
핵심: LLM의 언어 이해 + 로봇의 실제 수행 능력 결합

가능성 = P(유용|LLM) × P(성공|로봇)

LLM: "뭘 해야 하는가" (semantic)
Robot: "뭘 할 수 있는가" (affordance)
```

**영향:**
- LLM + 로봇 통합의 초기 핵심 연구
- 이후 많은 LLM-robot 연구의 기반

### RT-2 & VLA (2023)
```
핵심: VLM을 로봇 제어에 직접 활용

기존: 별도 perception + planning + control
RT-2: 단일 VLM이 이미지→액션 직접 출력
```

**영향:**
- VLA 패러다임 정립
- Foundation model의 로봇 적용

---

## Philosophy & Direction

### 연구 철학
> "로봇 AI의 핵심은 일반화. 특정 태스크가 아닌 범용 능력이 목표"

### 연구 방향 변화
1. **2012-2017**: Skill learning, multi-task RL
2. **2017-2022**: Large-scale robot learning at Google
3. **2022-2023**: LLM + robotics, VLA models
4. **2024-현재**: Foundation models, Physical Intelligence

---

## Google → Physical Intelligence

### Google에서의 성과
- RT 시리즈로 VLA 패러다임 정립
- SayCan으로 LLM-robot 통합 시작
- Google Robotics 핵심 인물

### 창업 동기
- 학술 연구를 넘어 실제 제품화
- 범용 로봇 AI 상용화
- 빠른 실행과 집중

---

## Links

- [Google Scholar](https://scholar.google.com/citations?user=XXXXXXXXX)
- [Physical Intelligence](https://physicalintelligence.company/)
- [Twitter/X](https://twitter.com/hauaborisov)

---

## See Also

- [RT](../models/rt.md)
- [π0](../models/pi0.md)
- [Physical Intelligence](../companies/physical-intelligence.md)
- [Sergey Levine](sergey-levine.md)
- [Chelsea Finn](chelsea-finn.md)
