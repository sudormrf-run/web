---
title: VLA Models
description: History and current state of Vision-Language-Action models
category: models
order: 1
isFeatured: true
icon: brain

# Authorship
createdBy:
  name: Jong Hyun Park
  email: jhpark@sudormrf.run
lastEditedBy:
  name: Jong Hyun Park
  email: jhpark@sudormrf.run
lastEditedAt: 2026-01-15
---

# VLA Model List

> History and list of Vision-Language-Action models

VLA (Vision-Language-Action) models are AI models that take visual information and language instructions to output robot actions.

---

## Key Models

| Model | Key Significance |
|-------|-----------------|
| RT-1/2 | Pioneer of VLA, established "Action as Language" paradigm |
| OpenVLA | First large-scale open-source VLA, democratizing research |
| π Series | Flow Matching, Open-world generalization, RL self-improvement |
| SmolVLA | π0-level with 450M, runs on MacBook |
| GR00T | First open humanoid foundation, proved synthetic data |

---

## Timeline

VLA models have evolved rapidly since RT-1 in 2022.

- 2022: RT-1 (Google)
- 2023: RT-2, ACT, Diffusion Policy
- 2024: Octo, OpenVLA, GR00T, π0
- 2025: SmolVLA, Gemini Robotics, π0.5
